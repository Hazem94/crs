{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Importing to be used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import  svm\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from gensim.parsing.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# A function used to remove stop words from a document passed as LIST\n",
    "def remove_stop_words(text):\n",
    "    for st_word in stop_words:\n",
    "        text = list(filter(lambda a: a != st_word, text))\n",
    "    return text\n",
    "\n",
    "# Function to stem, remove stop word, and tokenize documents \n",
    "def pre_process(doc, doc_id, is_train):\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # clean and tokenize document string    \n",
    "    tokens = utils.simple_preprocess(doc)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = remove_stop_words(tokens)\n",
    "\n",
    "    # remove numbers\n",
    "    number_tokens = [re.sub(r'[\\d]', ' ', i) for i in stopped_tokens]\n",
    "    number_tokens = ' '.join(number_tokens).split()\n",
    "\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(w) for w in number_tokens]\n",
    "\n",
    "    # If it's train data return TaggedDocument object else, return normal lists\n",
    "    if is_train == 0:\n",
    "        return stemmed_tokens\n",
    "    else:\n",
    "        return TaggedDocument(stemmed_tokens, [doc_id])\n",
    "            \n",
    "\n",
    "# Function used to divide both negative and positive text files into 2 parts, one for\n",
    "# test and the other for train. It returns two lists for train, test.\n",
    "def divide_corpus(p_corpus, n_corpus):\n",
    "    train_corpus = []\n",
    "    test_corpus = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "        \n",
    "    factor = 3 / 4\n",
    "    \n",
    "    n_pos = factor * len(p_corpus) \n",
    "    for i in range(len(p_corpus)):\n",
    "        if i < n_pos:\n",
    "            train_corpus.append(pre_process(p_corpus[i], i, 1))\n",
    "            train_labels.append([1])\n",
    "        else:\n",
    "            test_corpus.append(pre_process(p_corpus[i], i, 0))\n",
    "            test_labels.append([1])\n",
    "\n",
    "    n_neg = factor * len(n_corpus)\n",
    "    for i in range(len(n_corpus)):\n",
    "        if i < n_neg:\n",
    "            train_corpus.append(pre_process(n_corpus[i], int(n_pos+i), 1))\n",
    "            train_labels.append([0])\n",
    "        else:\n",
    "            test_corpus.append(pre_process(n_corpus[i], int(n_pos+i), 0))\n",
    "            test_labels.append([0])\n",
    "    return train_corpus, train_labels, test_corpus, test_labels\n",
    "\n",
    "# Function to shuffle corpus & labels in the same order\n",
    "def shuffle_corpus_labels(corpus, labels):\n",
    "    z = list(zip(corpus, labels))\n",
    "    random.shuffle(z)\n",
    "    c, l = zip(*z)\n",
    "    return c, l\n",
    "\n",
    "def prepare_classifier_train_arrays(model, labels_arr):\n",
    "    train_arrays = np.zeros((model.corpus_count, model.vector_size))\n",
    "    train_labels_arrays = np.zeros(model.corpus_count, dtype=int)\n",
    "    for i in range(model.corpus_count):\n",
    "        train_arrays[i] = model.docvecs[i]\n",
    "        train_labels_arrays[i] = labels_arr[i][0]\n",
    "    return train_arrays, train_labels_arrays\n",
    "\n",
    "def prepare_classifier_test_arrays(model, test_corpus, labels_arr):\n",
    "    test_arrays = np.zeros((len(test_corpus), model.vector_size))\n",
    "    test_labels_arrays = np.zeros(len(test_corpus), dtype=int)\n",
    "    \n",
    "    # Shuffle test data\n",
    "    test_corpus, labels_arr = shuffle_corpus_labels(test_corpus, labels_arr)\n",
    "\n",
    "    # Write test corpus & labels into pickle objects \n",
    "    with open('test_corpus.pickle', 'wb') as pkl:\n",
    "        pickle.dump(test_corpus, pkl)\n",
    "    with open('test_labels.pickle', 'wb') as pkl:\n",
    "        pickle.dump(labels_arr, pkl)\n",
    "        \n",
    "    for i in range(len(test_corpus)):\n",
    "        test_arrays[i] = model.infer_vector(test_corpus[i])\n",
    "        test_labels_arrays[i] = labels_arr[i][0]\n",
    "    return test_arrays, test_labels_arrays"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Reading pos and neg text files and building up the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 138 ms, sys: 81.9 ms, total: 220 ms\nWall time: 727 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First, download the data set from NLTK dataset website\n",
    "\n",
    "# The local path to the positively reviewed movies\n",
    "pos_rev_folder_path = \"/home/hazem/Downloads/movie_reviews/pos/\"\n",
    "\n",
    "# The local path to the negatively reviewed movies\n",
    "neg_rev_folder_path = \"/home/hazem/Downloads/movie_reviews/neg/\"\n",
    "\n",
    "# TaggedDocument: a Gensim object which has the form:\n",
    "# [TaggedDocument(words = ['w_1', 'w_2' ...], tags = [0]) 'doc_1', ...)\n",
    "\n",
    "# Positive text file as a list of lists\n",
    "pos_corpus = []\n",
    "\n",
    "# Negative text file as a list of lists\n",
    "neg_corpus = []\n",
    "\n",
    "# Iterator\n",
    "i = 0\n",
    "# Now, read the positive text files, filter out the stop words and convert all \n",
    "# letters to lowercase \n",
    "for file_name in os.listdir(pos_rev_folder_path):\n",
    "    with open(pos_rev_folder_path + file_name, 'r') as file:\n",
    "        doc = file.read().replace('\\n', '')\n",
    "        pos_corpus.append(doc)\n",
    "        i += 1\n",
    "        \n",
    "# Iterator \n",
    "i = 0\n",
    "# Now, read the negative text files, filter out the stop words and convert all \n",
    "# # letters to lowercase \n",
    "for file_name in os.listdir(neg_rev_folder_path):\n",
    "    with open(neg_rev_folder_path + file_name, 'r') as file:\n",
    "        doc = file.read().replace('\\n', '')\n",
    "        neg_corpus.append(doc)\n",
    "        i += 1\n",
    "\n",
    "# Write both pos and neg corpus & labels into pickle objects \n",
    "with open('pos_corpus.pickle', 'wb') as pkl:\n",
    "    pickle.dump(pos_corpus, pkl)\n",
    "\n",
    "with open('neg_corpus.pickle', 'wb') as pkl:\n",
    "    pickle.dump(neg_corpus, pkl)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Defining train and test datasets and write them into pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.7 s, sys: 78.7 ms, total: 34.8 s\nWall time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read from pickle objects\n",
    "with open('pos_corpus.pickle', 'rb') as pkl:\n",
    "    pos_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('neg_corpus.pickle', 'rb') as pkl:\n",
    "    neg_corpus = pickle.load(pkl)\n",
    "\n",
    "train_corpus, train_labels, test_corpus, test_labels = divide_corpus(pos_corpus, \n",
    "                                                                     neg_corpus)\n",
    "\n",
    "# Write both train and test corpus and labels into pickle objects \n",
    "with open('train_corpus.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_corpus, pkl)\n",
    "\n",
    "with open('train_labels.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_labels, pkl)\n",
    "\n",
    "with open('test_corpus.pickle', 'wb') as pkl:\n",
    "    pickle.dump(test_corpus, pkl)\n",
    "\n",
    "with open('test_labels.pickle', 'wb') as pkl:\n",
    "    pickle.dump(test_labels, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle objects\n",
    "with open('train_corpus.pickle', 'rb') as pkl:\n",
    "    train_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('train_labels.pickle', 'rb') as pkl:\n",
    "    train_labels = pickle.load(pkl)\n",
    "\n",
    "with open('test_corpus.pickle', 'rb') as pkl:\n",
    "    test_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('test_labels.pickle', 'rb') as pkl:\n",
    "    test_labels = pickle.load(pkl)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Now, building up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_0\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_1\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_2\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_3\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_4\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_5\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_6\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_7\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_8\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_9\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_10\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_11\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_12\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_13\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_14\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_15\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_16\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_17\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_18\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_19\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_20\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_21\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_22\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_23\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_24\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_25\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_26\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_27\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_28\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_29\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_30\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_31\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_32\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_33\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_34\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_35\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_36\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_37\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_38\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_39\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_40\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_41\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_42\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_43\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_44\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_45\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_46\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_47\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_48\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_49\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_50\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_51\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_52\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_53\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_54\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_55\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_56\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_57\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_58\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_59\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_60\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_61\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_62\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_63\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_64\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_65\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_66\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_67\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_68\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_69\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_70\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_71\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_72\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_73\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_74\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_75\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_76\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_77\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_78\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_79\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_80\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_81\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_82\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_83\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_84\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_85\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_86\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_87\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_88\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_89\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_90\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_91\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_92\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_93\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_94\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_95\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_96\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_97\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_98\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_99\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_100\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_101\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_102\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_103\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_104\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_105\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_106\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_107\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_108\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_109\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_110\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_111\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_112\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_113\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_114\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_115\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_116\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_117\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_118\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_119\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_120\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_121\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_122\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_123\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_124\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_125\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_126\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_127\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_128\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_129\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_130\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_131\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_132\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_133\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_134\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_135\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_136\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_137\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_138\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_139\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_140\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_141\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_142\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_143\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_144\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_145\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_146\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_147\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_148\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_149\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_150\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_151\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_152\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_153\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_154\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_155\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_156\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_157\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_158\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_159\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_160\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_161\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_162\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_163\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_164\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_165\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_166\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_167\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_168\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_169\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_170\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_171\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_172\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_173\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_174\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_175\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_176\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_177\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_178\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_179\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_180\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_181\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_182\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_183\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_184\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_185\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_186\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_187\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_188\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_189\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_190\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_191\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_192\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_193\t"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load from pickle objects\n",
    "with open('train_corpus.pickle', 'rb') as pkl:\n",
    "    train_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('train_labels.pickle', 'rb') as pkl:\n",
    "    train_labels = pickle.load(pkl)\n",
    "    \n",
    "# Model's parameter\n",
    "max_epochs = 200\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "# Note: defining 'dm=1' is important here. It means that we have selected \n",
    "# distributed memory’ (PV-DM) over ‘distributed bag of words’ (PV-DBOW) 'dm =0'\n",
    "# Which doesn't preserve teh order of the words.\n",
    "model = Doc2Vec(min_count=1, dm=1, sample=1e-4, negative=5, workers=16, \n",
    "                window=10, vector_size=vec_size, alpha=alpha, min_alpha=0.00025)\n",
    "\n",
    "train_corpus, train_labels = shuffle_corpus_labels(train_corpus, train_labels)\n",
    "\n",
    "# Setting up the vocabulary \n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    print('iteration_{0}'.format(epoch), end='\\t')\n",
    "        \n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "# Write both train corpus and labels into pickle objects \n",
    "with open('train_corpus.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_corpus, pkl)\n",
    "\n",
    "with open('train_labels.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_labels, pkl)\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"\\nModel Saved\\n\")\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 28 ms, total: 4.13 s\nWall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading the saved doc2vec model\n",
    "model = Doc2Vec.load('d2v.model')\n",
    "\n",
    "# Load from pickle objects\n",
    "with open('train_corpus.pickle', 'rb') as pkl:\n",
    "    train_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('train_labels.pickle', 'rb') as pkl:\n",
    "    train_labels = pickle.load(pkl)\n",
    "\n",
    "with open('test_corpus.pickle', 'rb') as pkl:\n",
    "    test_corpus = pickle.load(pkl)\n",
    "\n",
    "with open('test_labels.pickle', 'rb') as pkl:\n",
    "    test_labels = pickle.load(pkl)\n",
    "    \n",
    "    '''\n",
    "    print(train_corpus[:2])\n",
    "    print(train_labels[427], '\\t', train_labels[977])\n",
    "    '''\n",
    "    \n",
    "train_arrays, train_labels_arrays = \\\n",
    "    prepare_classifier_train_arrays(model, train_labels)\n",
    "\n",
    "test_arrays, test_labels_arrays = \\\n",
    "    prepare_classifier_test_arrays(model, test_corpus, test_labels)\n",
    "\n",
    "# Write both train and test arrays with labels into pi1ckle objects \n",
    "with open('train_arrays.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_arrays, pkl)\n",
    "with open('train_labels_arrays.pickle', 'wb') as pkl:\n",
    "    pickle.dump(train_labels_arrays, pkl)\n",
    "\n",
    "with open('test_arrays.pickle', 'wb') as pkl:\n",
    "    pickle.dump(test_arrays, pkl)\n",
    "with open('test_labels_arrays.pickle', 'wb') as pkl:\n",
    "    pickle.dump(test_labels_arrays, pkl)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from pickle objects\n",
    "with open('train_arrays.pickle', 'rb') as pkl:\n",
    "    train_x = pickle.load(pkl)\n",
    "with open('train_labels_arrays.pickle', 'rb') as pkl:\n",
    "    train_y = pickle.load(pkl)\n",
    "\n",
    "with open('test_arrays.pickle', 'rb') as pkl:\n",
    "    test_x = pickle.load(pkl)\n",
    "with open('test_labels_arrays.pickle', 'rb') as pkl:\n",
    "    test_y = pickle.load(pkl)\n",
    "    \n",
    "\n",
    "# Now we train a logistic regression classifier using the training data\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.464"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(gamma=1,C=0.01)\n",
    "clf.fit(train_x, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
