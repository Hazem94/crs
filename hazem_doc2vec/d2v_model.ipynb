{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Importing to be used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries for pre-processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, utils\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from hazem_doc2vec.helper_functions import in_pickle, out_pickle, shuffle_corpus_labels\n",
    "from data_analysis.preprocessor_end import preprocess_file\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import data_analysis.preprocessor_end as pre\n",
    "import os"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Defining function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to divide both negative and positive text files into 2 parts, one for\n",
    "# test and the other for train. It returns two lists for train, test.\n",
    "def divide_corpus(p_corpus, n_corpus, factor):\n",
    "    train_corpus = []\n",
    "    train_labels = []\n",
    "\n",
    "    test_corpus = []\n",
    "    test_labels = []\n",
    "\n",
    "    # For positive dataset\n",
    "    n_pos = int(math.ceil(factor * len(p_corpus)))\n",
    "    for doc_id in range(len(p_corpus)):\n",
    "        if doc_id < n_pos:\n",
    "            train_corpus.append(TaggedDocument(p_corpus[doc_id], [doc_id]))\n",
    "            train_labels.append([1])\n",
    "        else:\n",
    "            test_corpus.append(p_corpus[doc_id])\n",
    "            test_labels.append([1])\n",
    "    \n",
    "    # For negative dataset\n",
    "    n_neg = int(math.ceil(factor * len(n_corpus))) \n",
    "    for doc_id in range(len(n_corpus)):\n",
    "        if doc_id < n_neg:\n",
    "            train_corpus.append(TaggedDocument(n_corpus[doc_id], [int(n_pos + doc_id)]))\n",
    "            train_labels.append([0])\n",
    "        else:\n",
    "            test_corpus.append(n_corpus[doc_id])\n",
    "            test_labels.append([0])\n",
    "            \n",
    "    return train_corpus, train_labels, test_corpus, test_labels\n",
    "\n",
    "\n",
    "def prepare_classifier_train_arrays(model, labels_arr):\n",
    "    train_arrays = np.array(model.docvecs.vectors_docs)\n",
    "    train_labels_arrays = np.zeros(model.docvecs.count, dtype=np.int)\n",
    "    \n",
    "    for i in range(model.docvecs.count):\n",
    "        train_labels_arrays[i] = labels_arr[i][0]\n",
    "    return train_arrays, train_labels_arrays\n",
    "\n",
    "\n",
    "def prepare_classifier_test_arrays(model, test_corpus, labels_arr):\n",
    "    test_arrays = np.zeros([len(test_corpus), model.vector_size])\n",
    "    test_labels_arrays = np.zeros(len(test_corpus), dtype=np.int)\n",
    "\n",
    "    # Shuffle test data\n",
    "    test_corpus, labels_arr = shuffle_corpus_labels(test_corpus, labels_arr)\n",
    "\n",
    "    for i in range(len(test_corpus)):\n",
    "        test_arrays[i] = model.infer_vector(test_corpus[i])\n",
    "        test_labels_arrays[i] = labels_arr[i][0]\n",
    "    return test_arrays, test_labels_arrays"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Pickle in the negative and positive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 865 ms, total: 4.93 s\nWall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_corpus = in_pickle('data/pos_corpus')\n",
    "neg_corpus = in_pickle('data/neg_corpus')\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Pickle in the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.45 s, sys: 1.02 s, total: 5.47 s\nWall time: 5.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_corpus = in_pickle('data/train_corpus')\n",
    "train_labels = in_pickle('data/train_labels')\n",
    "test_corpus = in_pickle('data/test_corpus')\n",
    "test_labels = in_pickle('data/test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5036, 5036, 1258, 1258, 6294)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_corpus), len(train_labels), len(test_corpus), len(test_labels), len(pos_corpus) + len(neg_corpus))"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Now, building up the Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_0\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_1\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_2\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_3\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_4\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_5\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_6\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_7\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_8\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_9\t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nModel Saved\n\nCPU times: user 1h 46min 18s, sys: 35 s, total: 1h 46min 53s\nWall time: 31min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Model's parameter\n",
    "max_epochs = 10\n",
    "vec_size = 300\n",
    "alpha = 0.025\n",
    "\n",
    "# Note: defining 'dm=1' is important here. It means that we have selected \n",
    "# distributed memory’ (PV-DM) over ‘distributed bag of words’ (PV-DBOW) 'dm =0'\n",
    "# Which doesn't preserve teh order of the words.\n",
    "model = Doc2Vec(min_count=1, dm=1, workers=16, window=10, vector_size=vec_size, \n",
    "                alpha=alpha, min_alpha=0.00025)\n",
    "\n",
    "# Setting up the vocabulary \n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_corpus, train_labels = shuffle_corpus_labels(train_corpus, train_labels)\n",
    "\n",
    "    print('iteration_{0}'.format(epoch), end='\\t')\n",
    "\n",
    "    model.train(train_corpus, total_examples=len(train_corpus), epochs=model.epochs)\n",
    "        \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"\\nModel Saved\\n\")\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Inspecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 182 ms, total: 1min 14s\nWall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading the saved doc2vec model\n",
    "model = Doc2Vec.load('d2v.model')\n",
    "\n",
    "train_x, train_y = prepare_classifier_train_arrays(model, train_labels)\n",
    "test_x, test_y = prepare_classifier_test_arrays(model, test_corpus, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 3.98 ms, total: 15.9 ms\nWall time: 30.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_pickle('data/train_x', train_x)\n",
    "out_pickle('data/train_y', train_y)\n",
    "out_pickle('data/test_x', test_x)\n",
    "out_pickle('data/test_y', test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.03 ms, sys: 0 ns, total: 7.03 ms\nWall time: 6.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_x = in_pickle('data/train_x')\n",
    "train_y = in_pickle('data/train_y')\n",
    "test_x = in_pickle('data/test_x')\n",
    "test_y = in_pickle('data/test_y')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 598 ms, sys: 549 ms, total: 1.15 s\nWall time: 507 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "classifier.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7519872813990461"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 s, sys: 104 ms, total: 13.4 s\nWall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = svm.SVC(C=505, gamma=0.00055)\n",
    "clf.fit(train_x, train_y)   \n",
    "out_pickle('data/svm', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7170111287758346"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_x, test_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
